{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJmDTvufqAck",
        "outputId": "99702991-ff98-4765-fc0c-d0297a4f095e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 14.278231424154695\n",
            "Epoch [2/10], Loss: 10.164316496898218\n",
            "Epoch [3/10], Loss: 10.107071507837354\n",
            "Epoch [4/10], Loss: 10.029570677845749\n",
            "Epoch [5/10], Loss: 9.975398515917592\n",
            "Epoch [6/10], Loss: 10.034902267849322\n",
            "Epoch [7/10], Loss: 9.92331791415657\n",
            "Epoch [8/10], Loss: 9.897075156575626\n",
            "Epoch [9/10], Loss: 9.889410598990844\n",
            "Epoch [10/10], Loss: 9.873956041237742\n",
            "Accuracy on adversarial examples: 94.27234591803807%\n"
          ]
        }
      ],
      "source": [
        "# @title Réseau Maxout avec l'ajout de la fonction L1 Weight Decay\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "def FGSM(model, images, labels, epsilon):\n",
        "    images.requires_grad = True\n",
        "\n",
        "    outputs = model(images)\n",
        "    labels = labels.to(outputs.device)\n",
        "    loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "    model.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Génération des perturbations adversariales\n",
        "    perturbation = epsilon * images.grad.sign()\n",
        "    adversarial_images = images + perturbation\n",
        "    adversarial_images = torch.clamp(adversarial_images, 0, 1)\n",
        "    return adversarial_images\n",
        "\n",
        "# Classe Maxout\n",
        "class Maxout(nn.Module):\n",
        "    def __init__(self, in_features, out_features, num_pieces):\n",
        "        super(Maxout, self).__init__()\n",
        "        self.num_pieces = num_pieces\n",
        "        self.linear = nn.Linear(in_features, out_features * num_pieces)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = x.view(x.size(0), -1, self.num_pieces)\n",
        "\n",
        "        return x.max(-1)[0]\n",
        "\n",
        "# Classe du modèle (réseau profond avec maxout)\n",
        "class DeepMaxoutNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepMaxoutNN, self).__init__()\n",
        "        self.maxout1 = Maxout(28 * 28, 256, 4)  # Maxout avec 4 morceaux\n",
        "        self.maxout2 = Maxout(256, 128, 4)\n",
        "        self.fc = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = self.maxout1(x)\n",
        "        x = self.maxout2(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Charger les données MNIST (avec uniquement des 3 et des 7)\n",
        "def load_data():\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    full_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "    indices = [i for i in range(len(full_dataset)) if full_dataset.targets[i] == 3 or full_dataset.targets[i] == 7]\n",
        "    filtered_dataset = Subset(full_dataset, indices)\n",
        "\n",
        "    train_loader = DataLoader(filtered_dataset, batch_size=64, shuffle=True)\n",
        "    return train_loader\n",
        "\n",
        "# Entraînement sur des données normales\n",
        "def train_on_normal_data(model, train_loader, optimizer, epochs=10, l1_lambda=0.0):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "            # Ajout de la régularisation L_1 sur les poids de la première couche uniquement\n",
        "            if l1_lambda > 0.0:\n",
        "                l1_norm = torch.norm(model.maxout1.linear.weight, p=1)\n",
        "                loss += l1_lambda * l1_norm\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Test sur des données adversariales\n",
        "def test_on_adversarial_data(model, train_loader, epsilon):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.clone().detach().requires_grad_(True)\n",
        "        adversarial_images = FGSM(model, images, labels, epsilon)\n",
        "\n",
        "        outputs = model(adversarial_images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy on adversarial examples: {accuracy}%')\n",
        "    return accuracy\n",
        "\n",
        "# Fonction principale\n",
        "def main():\n",
        "    # Paramètres\n",
        "    epsilon = 0.025  # Magnitude des perturbations adversariales\n",
        "    epochs = 10 # Nombre d'époques d'entraînement\n",
        "    l1_lambda = 0.01  # Coefficient de régularisation L1\n",
        "\n",
        "    train_loader = load_data()\n",
        "\n",
        "    model = DeepMaxoutNN()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    train_on_normal_data(model, train_loader, optimizer, epochs, l1_lambda)\n",
        "\n",
        "    accuracy = test_on_adversarial_data(model, train_loader, epsilon)\n",
        "\n",
        "# Exécuter la fonction principale\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On observe un immense gain de performances ! Cependant, comme il est dit dans l'article, si on prend un lambda supérieur à epsilon, le réseau perd en performances."
      ],
      "metadata": {
        "id": "2HPiSebSwI-h"
      }
    }
  ]
}
