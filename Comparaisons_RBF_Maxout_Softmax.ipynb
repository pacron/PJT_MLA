{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dans ce code, on essaie de s'intéresser à pourquoi différents réseaux neuronaux classificateurs assignent la même classe à un exemple adversarial. L'hypothèse première est que c'est la linéarité de la méthode d'entraînement qui amènerait différents réseaux à classifier de la même manière les exemples adversariaux, en effet, les algorithmes de Machine Learning classifient assez bien pour généraliser."
      ],
      "metadata": {
        "id": "lwNoCQ7YA2sa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA2_TdG2pqdk",
        "outputId": "83edbe38-4ce9-4a62-f4b3-ec5407f29412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "Maxout vs Softmax:\n",
            " - Les deux modèles se trompent : 97.67%\n",
            " - Accord sur les erreurs : 55.37%\n",
            "Maxout vs RBF:\n",
            " - Les deux modèles se trompent : 50.69%\n",
            " - Accord sur les erreurs : 20.64%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Charger les données MNIST\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "x_train_flat = x_train.reshape(-1, 28 * 28)\n",
        "x_test_flat = x_test.reshape(-1, 28 * 28)\n",
        "\n",
        "# Fonction pour générer des exemples adversariaux (FGSM)\n",
        "def generate_adversarial_examples(model, x, y, epsilon=0.25):\n",
        "    adv_examples = []\n",
        "    for i in range(len(x)):\n",
        "        x_sample = tf.convert_to_tensor(x[i].reshape(1, 28, 28), dtype=tf.float32)\n",
        "        y_sample = tf.convert_to_tensor([y[i]], dtype=tf.int64)\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(x_sample)\n",
        "            pred = model(x_sample)\n",
        "            loss = tf.keras.losses.sparse_categorical_crossentropy(y_sample, pred)\n",
        "        gradient = tape.gradient(loss, x_sample)\n",
        "        perturbation = epsilon * tf.sign(gradient)\n",
        "        x_adv = x_sample + perturbation\n",
        "        x_adv = tf.clip_by_value(x_adv, 0, 1)\n",
        "        adv_examples.append(x_adv.numpy().squeeze())\n",
        "    return np.array(adv_examples)\n",
        "\n",
        "# Modèle Maxout simulé avec Dense\n",
        "def create_maxout_model():\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Modèle Softmax peu profond\n",
        "def create_softmax_model():\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Modèle RBF\n",
        "class RBFModel:\n",
        "    def __init__(self, num_classes=10):\n",
        "        self.num_classes = num_classes\n",
        "        self.kmeans = KMeans(n_clusters=num_classes)\n",
        "        self.log_reg = LogisticRegression(max_iter=1000)\n",
        "\n",
        "    def fit(self, x, y):\n",
        "        self.kmeans.fit(x)\n",
        "        rbf_features = rbf_kernel(x, self.kmeans.cluster_centers_)\n",
        "        self.log_reg.fit(rbf_features, y)\n",
        "\n",
        "    def predict(self, x):\n",
        "        rbf_features = rbf_kernel(x, self.kmeans.cluster_centers_)\n",
        "        return self.log_reg.predict(rbf_features)\n",
        "\n",
        "# Entraînement des modèles\n",
        "model_maxout = create_maxout_model()\n",
        "model_maxout.fit(x_train, y_train, epochs=5, verbose=0)\n",
        "\n",
        "model_softmax = create_softmax_model()\n",
        "model_softmax.fit(x_train, y_train, epochs=5, verbose=0)\n",
        "\n",
        "model_rbf = RBFModel()\n",
        "model_rbf.fit(x_train_flat, y_train)\n",
        "\n",
        "# Génération des exemples adversariaux avec le modèle Maxout\n",
        "x_adv = generate_adversarial_examples(model_maxout, x_test, y_test)\n",
        "\n",
        "# Prédictions croisées\n",
        "y_pred_maxout = np.argmax(model_maxout.predict(x_adv), axis=1)\n",
        "y_pred_softmax = np.argmax(model_softmax.predict(x_adv), axis=1)\n",
        "y_pred_rbf = model_rbf.predict(x_adv.reshape(-1, 28 * 28))\n",
        "\n",
        "# Calcul des pourcentages\n",
        "def calculate_statistics(y_true, y_pred_1, y_pred_2):\n",
        "    both_wrong = (y_true != y_pred_1) & (y_true != y_pred_2)\n",
        "    agree_when_wrong = both_wrong & (y_pred_1 == y_pred_2)\n",
        "    return {\n",
        "        \"both_wrong\": np.mean(both_wrong) * 100,\n",
        "        \"agree_when_wrong\": np.mean(agree_when_wrong) * 100,\n",
        "    }\n",
        "\n",
        "# Résultats pour Maxout vs Softmax\n",
        "stats_maxout_softmax = calculate_statistics(y_test, y_pred_maxout, y_pred_softmax)\n",
        "# Résultats pour Maxout vs RBF\n",
        "stats_maxout_rbf = calculate_statistics(y_test, y_pred_maxout, y_pred_rbf)\n",
        "\n",
        "# Affichage des résultats\n",
        "print(\"Maxout vs Softmax:\")\n",
        "print(f\" - Les deux modèles se trompent : {stats_maxout_softmax['both_wrong']:.2f}%\")\n",
        "print(f\" - Accord sur les erreurs : {stats_maxout_softmax['agree_when_wrong']:.2f}%\")\n",
        "\n",
        "print(\"Maxout vs RBF:\")\n",
        "print(f\" - Les deux modèles se trompent : {stats_maxout_rbf['both_wrong']:.2f}%\")\n",
        "print(f\" - Accord sur les erreurs : {stats_maxout_rbf['agree_when_wrong']:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ici, le réseau softmax (principalement linéaire) a une forte tendance à reproduire les prédictions du réseau Maxout, ce qui montre l'importance des composantes linéaires dans le processus de généralisation des modèles, tandis que le réseau RBF lui (moins linéaire) est bien plus éloigné du réseau Maxout."
      ],
      "metadata": {
        "id": "HBTunKB8C24e"
      }
    }
  ]
}