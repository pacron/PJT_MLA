import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
import torchvision.models as models
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader
import numpy as np
import matplotlib.patches as patches
import matplotlib.pyplot as plt

torch.save(model.state_dict(), 'cifar10_resnet18.pth')


# Load CIFAR-10 class names
classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']

# Load a pre-trained model (for demonstration, we use a ResNet18 pre-trained on CIFAR-10)
model = models.resnet18(pretrained=False, num_classes=10)
model.load_state_dict(torch.load('cifar10_resnet18.pth'))  # Assumes model weights are saved locally
model.eval()

def generate_fooling_image(model, target_class, input_size=(3, 32, 32), confidence_threshold=0.5):
    # Initialisation gaussienne ou basée sur des images réelles
    image = torch.randn(1, *input_size, requires_grad=True)  # Option : Charger une image réelle
    
    optimizer = optim.Adam([image], lr=0.1)  # Augmenté le taux d'apprentissage
    
    for step in range(1000):  # Augmenté le nombre d'itérations
        optimizer.zero_grad()
        output = model(image)
        target_score = output[0, target_class]
        loss = -target_score
        loss.backward()
        optimizer.step()
        image.data.clamp_(0, 1)  # Maintenir les pixels dans une plage valide
        probabilities = nn.Softmax(dim=1)(output)
        if probabilities[0, target_class] > confidence_threshold:
            return image.detach(), probabilities[0, target_class].item()
    
    return image.detach(), probabilities[0, target_class].item()


# Generate fooling images for the "airplane" class
fooling_images = []
successful_images = []
num_samples = 10
for _ in range(num_samples):
    fooling_image, confidence = generate_fooling_image(model, target_class=0)
    fooling_images.append((fooling_image.squeeze().permute(1, 2, 0).numpy(), confidence))
    if confidence >= 0.5:
        successful_images.append(fooling_image.squeeze().permute(1, 2, 0).numpy())

# Plot results
fig, axes = plt.subplots(1, num_samples, figsize=(15, 5))
for i, (img, confidence) in enumerate(fooling_images):
    ax = axes[i]
    ax.imshow(img)
    ax.axis('off')
    if confidence >= 0.5:
        # Add a yellow box around successful images
        rect = patches.Rectangle((0, 0), 32, 32, linewidth=2, edgecolor='yellow', facecolor='none')
        ax.add_patch(rect)
    ax.set_title(f'{confidence:.2f}')

plt.suptitle("Generated Fooling Images for 'Airplane' Class")
plt.show()

# Display successful fooling images
if successful_images:
    num_success = len(successful_images)
    fig, axes = plt.subplots(1, num_success, figsize=(15, 5))
    
    # Ensure `axes` is iterable (wrap in a list if only one image)
    if num_success == 1:
        axes = [axes]
    
    for i, img in enumerate(successful_images):
        ax = axes[i]
        ax.imshow(img)
        ax.axis('off')
        ax.set_title("Fooled")
    
    plt.suptitle("Successful Fooling Images")
    plt.show()
else:
    print("No successful fooling images were generated.")


import numpy as np
from sklearn.metrics import accuracy_score
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# Generate Gaussian rubbish data for MNIST and CIFAR-10
np.random.seed(42)
def generate_gaussian_data(samples, dimensions):
    return np.random.normal(0, 1, (samples, dimensions))

# Define synthetic data
mnist_samples = 10000
mnist_dimensions = 784
cifar_samples = 1000
cifar_dimensions = 3072

mnist_rubbish_data = generate_gaussian_data(mnist_samples, mnist_dimensions)
cifar_rubbish_data = generate_gaussian_data(cifar_samples, cifar_dimensions)

# Simulated labels (for evaluation purposes)
mnist_labels = np.random.randint(0, 10, size=(mnist_samples,))
cifar_labels = np.random.randint(0, 10, size=(cifar_samples,))

# PyTorch Model Definitions
class MaxoutNetwork(nn.Module):
    def __init__(self, input_dim, output_dim, num_units=5):
        super(MaxoutNetwork, self).__init__()
        self.fc = nn.Linear(input_dim, num_units * output_dim)
        self.output_dim = output_dim
        self.num_units = num_units

    def forward(self, x):
        x = self.fc(x)
        x = x.view(-1, self.output_dim, self.num_units)
        x, _ = torch.max(x, dim=2)
        return x

class SigmoidNetwork(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(SigmoidNetwork, self).__init__()
        self.fc = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        return torch.sigmoid(self.fc(x))

# Utility Functions
def evaluate_model(model, data, labels, threshold=0.5):
    model.eval()
    with torch.no_grad():
        inputs = torch.tensor(data, dtype=torch.float32)
        logits = model(inputs)
        probabilities = torch.softmax(logits, dim=1)
        confidences, predictions = torch.max(probabilities, dim=1)
        errors = predictions.numpy() != labels
        error_rate = np.mean(errors)
        avg_confidence = np.mean(confidences.numpy()[errors])
        return error_rate, avg_confidence

# Prepare Models
mnist_maxout = MaxoutNetwork(mnist_dimensions, 10)
mnist_sigmoid = SigmoidNetwork(mnist_dimensions, 10)
cifar_maxout = MaxoutNetwork(cifar_dimensions, 10)

# Define Example Evaluations (assuming models are trained)
def evaluate_rubbish_data():
    mnist_error_rate_maxout, mnist_avg_conf_maxout = evaluate_model(
        mnist_maxout, mnist_rubbish_data, mnist_labels
    )
    mnist_error_rate_sigmoid, mnist_avg_conf_sigmoid = evaluate_model(
        mnist_sigmoid, mnist_rubbish_data, mnist_labels
    )

    cifar_error_rate_maxout, cifar_avg_conf_maxout = evaluate_model(
        cifar_maxout, cifar_rubbish_data, cifar_labels
    )

    print(f"MNIST Maxout Error Rate: {mnist_error_rate_maxout * 100:.2f}%, Avg Confidence: {mnist_avg_conf_maxout * 100:.2f}%")
    print(f"MNIST Sigmoid Error Rate: {mnist_error_rate_sigmoid * 100:.2f}%, Avg Confidence: {mnist_avg_conf_sigmoid * 100:.2f}%")
    print(f"CIFAR Maxout Error Rate: {cifar_error_rate_maxout * 100:.2f}%, Avg Confidence: {cifar_avg_conf_maxout * 100:.2f}%")

# Evaluate (use pre-trained weights or random initialization for demonstration)
evaluate_rubbish_data()


import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
import torch
import torch.nn as nn

# Génération de données gaussiennes
np.random.seed(42)
def generate_gaussian_data(samples, dimensions):
    return np.random.normal(0, 1, (samples, dimensions))

# Configuration des données synthétiques
mnist_samples = 10000
mnist_dimensions = 784

mnist_rubbish_data = generate_gaussian_data(mnist_samples, mnist_dimensions)
mnist_labels = np.random.randint(0, 10, size=(mnist_samples,))

# Modèle de régression softmax (logistic regression multiclasses)
def evaluate_softmax():
    softmax_model = LogisticRegression(max_iter=1000, multi_class='multinomial')
    
    # Entraînement sur des données aléatoires (simulées)
    training_data = generate_gaussian_data(5000, mnist_dimensions)
    training_labels = np.random.randint(0, 10, size=(5000,))
    softmax_model.fit(training_data, training_labels)

    # Prédictions sur les données "rubbish"
    probabilities = softmax_model.predict_proba(mnist_rubbish_data)
    predictions = np.argmax(probabilities, axis=1)
    confidences = np.max(probabilities, axis=1)
    errors = predictions != mnist_labels

    error_rate = np.mean(errors)
    avg_confidence = np.mean(confidences[errors]) if np.any(errors) else None
    return error_rate, avg_confidence

# Réseau à base de fonctions RBF
def evaluate_rbf():
    kernel = RBF(length_scale=1.0)
    rbf_model = GaussianProcessClassifier(kernel=kernel)

    # Entraînement sur des données aléatoires (simulées)
    training_data = generate_gaussian_data(500, mnist_dimensions)
    training_labels = np.random.randint(0, 10, size=(500,))
    rbf_model.fit(training_data, training_labels)

    # Prédictions sur les données "rubbish"
    predictions = rbf_model.predict(mnist_rubbish_data)
    probabilities = rbf_model.predict_proba(mnist_rubbish_data)
    confidences = np.max(probabilities, axis=1)
    errors = predictions != mnist_labels

    error_rate = np.mean(errors)
    avg_confidence = np.mean(confidences[errors]) if np.any(errors) else None
    return error_rate, avg_confidence

# Évaluation des modèles
softmax_error_rate, softmax_avg_conf = evaluate_softmax()
print(f"Softmax Regression Error Rate: {softmax_error_rate * 100:.2f}%")
if softmax_avg_conf is not None:
    print(f"Softmax Avg Confidence on Mistakes: {softmax_avg_conf * 100:.2f}%")
else:
    print("Softmax Avg Confidence on Mistakes: Undefined")

rbf_error_rate, rbf_avg_conf = evaluate_rbf()
print(f"RBF Network Error Rate: {rbf_error_rate * 100:.2f}%")
if rbf_avg_conf is not None:
    print(f"RBF Avg Confidence on Mistakes: {rbf_avg_conf * 100:.2f}%")
else:
    print("RBF Avg Confidence on Mistakes: Undefined")
